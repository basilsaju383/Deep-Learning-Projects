{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMq5Dy/lfDwSbWOvPbFNrJz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/basilsaju383/Deep-Learning-Projects/blob/main/keras_Tuner_anomaly_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_Le4NHrOk4ZJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('/content/sample_data/mc4 GSM 3 month.xlsx')\n",
        "df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
        "df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
        "df = df[['DateTime', 'RSSI']]\n",
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "q6iGygB7m0yu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timestamps = df['DateTime']\n",
        "\n",
        "# Use both Temperature and Humidity columns\n",
        "data = df[['RSSI']].values\n",
        "\n",
        "# Normalize the data to the range [0, 1]\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "# Convert timestamps to Unix timestamps\n",
        "unix_timestamps = timestamps.astype(np.int64) // 10**9  # Convert nanoseconds to seconds\n",
        "\n",
        "# Concatenate Unix timestamps with the normalized data\n",
        "data_with_timestamps = np.concatenate((unix_timestamps.values.reshape(-1, 1), data_scaled), axis=1)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data, test_data = train_test_split(data_with_timestamps, test_size=0.2, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "pZKdXB0crKJ7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(data, seq_length):\n",
        "    X = []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        seq = data[i:i+seq_length]\n",
        "        target = data[i+seq_length, 0]  # Temperature is the target variable\n",
        "        X.append(seq)\n",
        "    return np.array(X)\n",
        "# Define the sequence length\n",
        "seq_length = 10\n",
        "\n",
        "# Create sequences for training and testing for both temperature and humidity\n",
        "X_train = create_sequences(train_data[:, 1:], seq_length)  # Exclude timestamps for input data\n",
        "X_test = create_sequences(test_data[:, 1:], seq_length)"
      ],
      "metadata": {
        "id": "SRx3N8Y_rKMe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape,X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Me5ZHrQGm01z",
        "outputId": "38782ee6-0914-4e6d-8bb0-c443c37f86db"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3713, 10, 1), (921, 10, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner -q"
      ],
      "metadata": {
        "id": "-0FLLpIjvUMO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda65607-5a83-4003-b10d-0cc975874c86"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m122.9/129.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "import tensorflow as tf\n",
        "import keras_tuner\n",
        "\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Tune the number of layers.\n",
        "    for i in range(hp.Int(\"num_layers\", 1, 10)):\n",
        "        model.add(\n",
        "            layers.Dense(\n",
        "                # Tune number of units separately.\n",
        "                units=hp.Int(f\"units_{i}\", min_value=32, max_value=512, step=32),\n",
        "                activation=hp.Choice(\"activation\", [\"relu\", \"tanh\", \"sigmoid\"]),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    if hp.Boolean(\"dropout\"):\n",
        "        model.add(layers.Dropout(rate=0.25))\n",
        "\n",
        "    model.add(layers.Dense(10, activation=hp.Choice(\"output_activation\", [\"relu\", \"tanh\", \"sigmoid\"])))\n",
        "\n",
        "    optimizer_name = hp.Choice(\"optimizer\", values=[\"adam\", \"adadelta\", \"sgd\", \"rmsprop\"])\n",
        "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
        "\n",
        "    # Create an instance of the selected optimizer\n",
        "    if optimizer_name == \"adam\":\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    elif optimizer_name == \"adadelta\":\n",
        "        optimizer = tf.keras.optimizers.Adadelta(learning_rate=learning_rate)\n",
        "    elif optimizer_name == \"sgd\":\n",
        "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "    elif optimizer_name == \"rmsprop\":\n",
        "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid optimizer name\")\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"mae\",\n",
        "        metrics=[\"mae\"],\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "build_model(keras_tuner.HyperParameters())\n"
      ],
      "metadata": {
        "id": "qrTnFsCFvUPM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbd0ada7-c441-46c7-c2c7-381ff84a3ce8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.engine.sequential.Sequential at 0x7ba67bc2c190>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = keras_tuner.RandomSearch(\n",
        "    hypermodel=build_model,\n",
        "    objective=\"val_loss\",\n",
        "    max_trials=3,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMhpkR_N-ndS",
        "outputId": "7d9d84a0-fc84-4a34-db5e-3f73d7b16f43"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reloading Tuner from ./untitled_project/tuner0.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search_space_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eY8FlmFB-nnn",
        "outputId": "df6c0876-8343-4cba-e65f-43d5b2d8e941"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space summary\n",
            "Default search space size: 12\n",
            "num_layers (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 10, 'step': 1, 'sampling': 'linear'}\n",
            "units_0 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
            "activation (Choice)\n",
            "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh', 'sigmoid'], 'ordered': False}\n",
            "dropout (Boolean)\n",
            "{'default': False, 'conditions': []}\n",
            "output_activation (Choice)\n",
            "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh', 'sigmoid'], 'ordered': False}\n",
            "optimizer (Choice)\n",
            "{'default': 'adam', 'conditions': [], 'values': ['adam', 'adadelta', 'sgd', 'rmsprop'], 'ordered': False}\n",
            "lr (Float)\n",
            "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n",
            "units_1 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
            "units_2 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
            "units_3 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
            "units_4 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
            "units_5 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(X_train, X_train, epochs=5, validation_data=(X_test, X_test))"
      ],
      "metadata": {
        "id": "uP-b_UpN-nsH"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.results_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTg943yeCn6L",
        "outputId": "4c907ec1-91d6-4475-94f5-09ef6de24a14"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results summary\n",
            "Results in ./untitled_project\n",
            "Showing 10 best trials\n",
            "Objective(name=\"val_loss\", direction=\"min\")\n",
            "\n",
            "Trial 2 summary\n",
            "Hyperparameters:\n",
            "num_layers: 5\n",
            "units_0: 256\n",
            "activation: tanh\n",
            "dropout: False\n",
            "output_activation: sigmoid\n",
            "optimizer: adadelta\n",
            "lr: 0.00020422632013227844\n",
            "units_1: 320\n",
            "units_2: 320\n",
            "units_3: 416\n",
            "units_4: 160\n",
            "units_5: 384\n",
            "Score: 0.4458969533443451\n",
            "\n",
            "Trial 1 summary\n",
            "Hyperparameters:\n",
            "num_layers: 1\n",
            "units_0: 64\n",
            "activation: tanh\n",
            "dropout: True\n",
            "output_activation: relu\n",
            "optimizer: rmsprop\n",
            "lr: 0.004226349752062374\n",
            "units_1: 288\n",
            "units_2: 416\n",
            "units_3: 64\n",
            "units_4: 160\n",
            "units_5: 96\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n",
            "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n",
            "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n",
            "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n",
            "    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n",
            "    return model.fit(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/tmp/__autograph_generated_filejp4952zi.py\", line 15, in tf__train_function\n",
            "    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\n",
            "TypeError: in user code:\n",
            "\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n",
            "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n",
            "        outputs = model.train_step(data)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1155, in train_step\n",
            "        return self.compute_metrics(x, y, y_pred, sample_weight)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1249, in compute_metrics\n",
            "        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 620, in update_state\n",
            "        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/metrics_utils.py\", line 77, in decorated\n",
            "        result = update_state_fn(*args, **kwargs)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n",
            "        return ag_update_state(*args, **kwargs)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py\", line 723, in update_state  **\n",
            "        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n",
            "\n",
            "    TypeError: 'str' object is not callable\n",
            "\n",
            "\n",
            "\n",
            "Trial 0 summary\n",
            "Hyperparameters:\n",
            "num_layers: 6\n",
            "units_0: 320\n",
            "activation: tanh\n",
            "dropout: False\n",
            "output_activation: tanh\n",
            "optimizer: adadelta\n",
            "lr: 0.00025969686397759293\n",
            "units_1: 32\n",
            "units_2: 32\n",
            "units_3: 32\n",
            "units_4: 32\n",
            "units_5: 32\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n",
            "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n",
            "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n",
            "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n",
            "    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n",
            "    return model.fit(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/tmp/__autograph_generated_filejp4952zi.py\", line 15, in tf__train_function\n",
            "    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\n",
            "TypeError: in user code:\n",
            "\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n",
            "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n",
            "        outputs = model.train_step(data)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1155, in train_step\n",
            "        return self.compute_metrics(x, y, y_pred, sample_weight)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1249, in compute_metrics\n",
            "        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 620, in update_state\n",
            "        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/metrics_utils.py\", line 77, in decorated\n",
            "        result = update_state_fn(*args, **kwargs)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n",
            "        return ag_update_state(*args, **kwargs)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py\", line 723, in update_state  **\n",
            "        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n",
            "\n",
            "    TypeError: 'str' object is not callable\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.get_best_hyperparameters()[0].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CA5nXABiCn9L",
        "outputId": "4b9fd768-3b41-4379-c81c-320d969c9898"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'num_layers': 5,\n",
              " 'units_0': 256,\n",
              " 'activation': 'tanh',\n",
              " 'dropout': False,\n",
              " 'output_activation': 'sigmoid',\n",
              " 'optimizer': 'adadelta',\n",
              " 'lr': 0.00020422632013227844,\n",
              " 'units_1': 320,\n",
              " 'units_2': 320,\n",
              " 'units_3': 416,\n",
              " 'units_4': 160,\n",
              " 'units_5': 384}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tuner.get_best_models(num_models=1)[0]"
      ],
      "metadata": {
        "id": "uIDOQMOICoDE"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, X_train ,epochs=500, initial_epoch=5, validation_data=(X_test,X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS7n4Ej5FVGf",
        "outputId": "7405dbd0-a5ae-4333-adfc-1f0debf98fbd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/500\n",
            "117/117 [==============================] - 3s 18ms/step - loss: 0.4454 - mae: 0.4454 - val_loss: 0.4454 - val_mae: 0.4454\n",
            "Epoch 7/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.4449 - mae: 0.4449 - val_loss: 0.4448 - val_mae: 0.4448\n",
            "Epoch 8/500\n",
            "117/117 [==============================] - 1s 9ms/step - loss: 0.4443 - mae: 0.4443 - val_loss: 0.4443 - val_mae: 0.4443\n",
            "Epoch 9/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.4437 - mae: 0.4437 - val_loss: 0.4437 - val_mae: 0.4437\n",
            "Epoch 10/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.4431 - mae: 0.4431 - val_loss: 0.4431 - val_mae: 0.4431\n",
            "Epoch 11/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.4425 - mae: 0.4425 - val_loss: 0.4424 - val_mae: 0.4424\n",
            "Epoch 12/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.4419 - mae: 0.4419 - val_loss: 0.4418 - val_mae: 0.4418\n",
            "Epoch 13/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.4413 - mae: 0.4413 - val_loss: 0.4412 - val_mae: 0.4412\n",
            "Epoch 14/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.4406 - mae: 0.4406 - val_loss: 0.4405 - val_mae: 0.4405\n",
            "Epoch 15/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.4399 - mae: 0.4399 - val_loss: 0.4398 - val_mae: 0.4398\n",
            "Epoch 16/500\n",
            "117/117 [==============================] - 2s 16ms/step - loss: 0.4392 - mae: 0.4392 - val_loss: 0.4391 - val_mae: 0.4391\n",
            "Epoch 17/500\n",
            "117/117 [==============================] - 1s 9ms/step - loss: 0.4385 - mae: 0.4385 - val_loss: 0.4384 - val_mae: 0.4384\n",
            "Epoch 18/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.4378 - mae: 0.4378 - val_loss: 0.4377 - val_mae: 0.4377\n",
            "Epoch 19/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.4371 - mae: 0.4371 - val_loss: 0.4369 - val_mae: 0.4369\n",
            "Epoch 20/500\n",
            "117/117 [==============================] - 1s 9ms/step - loss: 0.4363 - mae: 0.4363 - val_loss: 0.4362 - val_mae: 0.4362\n",
            "Epoch 21/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.4356 - mae: 0.4356 - val_loss: 0.4354 - val_mae: 0.4354\n",
            "Epoch 22/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.4348 - mae: 0.4348 - val_loss: 0.4346 - val_mae: 0.4346\n",
            "Epoch 23/500\n",
            "117/117 [==============================] - 1s 9ms/step - loss: 0.4340 - mae: 0.4340 - val_loss: 0.4339 - val_mae: 0.4339\n",
            "Epoch 24/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.4332 - mae: 0.4332 - val_loss: 0.4330 - val_mae: 0.4330\n",
            "Epoch 25/500\n",
            "117/117 [==============================] - 2s 15ms/step - loss: 0.4324 - mae: 0.4324 - val_loss: 0.4322 - val_mae: 0.4322\n",
            "Epoch 26/500\n",
            "117/117 [==============================] - 2s 16ms/step - loss: 0.4316 - mae: 0.4316 - val_loss: 0.4314 - val_mae: 0.4314\n",
            "Epoch 27/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.4308 - mae: 0.4308 - val_loss: 0.4306 - val_mae: 0.4306\n",
            "Epoch 28/500\n",
            "117/117 [==============================] - 1s 9ms/step - loss: 0.4299 - mae: 0.4299 - val_loss: 0.4297 - val_mae: 0.4297\n",
            "Epoch 29/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.4290 - mae: 0.4290 - val_loss: 0.4288 - val_mae: 0.4288\n",
            "Epoch 30/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.4282 - mae: 0.4282 - val_loss: 0.4280 - val_mae: 0.4280\n",
            "Epoch 31/500\n",
            "117/117 [==============================] - 1s 9ms/step - loss: 0.4273 - mae: 0.4273 - val_loss: 0.4271 - val_mae: 0.4271\n",
            "Epoch 32/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.4264 - mae: 0.4264 - val_loss: 0.4262 - val_mae: 0.4262\n",
            "Epoch 33/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.4255 - mae: 0.4255 - val_loss: 0.4252 - val_mae: 0.4252\n",
            "Epoch 34/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.4245 - mae: 0.4245 - val_loss: 0.4243 - val_mae: 0.4243\n",
            "Epoch 35/500\n",
            "117/117 [==============================] - 2s 18ms/step - loss: 0.4236 - mae: 0.4236 - val_loss: 0.4234 - val_mae: 0.4234\n",
            "Epoch 36/500\n",
            "117/117 [==============================] - 2s 20ms/step - loss: 0.4227 - mae: 0.4227 - val_loss: 0.4224 - val_mae: 0.4224\n",
            "Epoch 37/500\n",
            "117/117 [==============================] - 3s 22ms/step - loss: 0.4217 - mae: 0.4217 - val_loss: 0.4214 - val_mae: 0.4214\n",
            "Epoch 38/500\n",
            "117/117 [==============================] - 3s 22ms/step - loss: 0.4207 - mae: 0.4207 - val_loss: 0.4205 - val_mae: 0.4205\n",
            "Epoch 39/500\n",
            "117/117 [==============================] - 2s 20ms/step - loss: 0.4197 - mae: 0.4197 - val_loss: 0.4195 - val_mae: 0.4195\n",
            "Epoch 40/500\n",
            "117/117 [==============================] - 3s 22ms/step - loss: 0.4187 - mae: 0.4187 - val_loss: 0.4184 - val_mae: 0.4184\n",
            "Epoch 41/500\n",
            "117/117 [==============================] - 3s 24ms/step - loss: 0.4177 - mae: 0.4177 - val_loss: 0.4174 - val_mae: 0.4174\n",
            "Epoch 42/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.4167 - mae: 0.4167 - val_loss: 0.4164 - val_mae: 0.4164\n",
            "Epoch 43/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.4156 - mae: 0.4156 - val_loss: 0.4153 - val_mae: 0.4153\n",
            "Epoch 44/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.4146 - mae: 0.4146 - val_loss: 0.4143 - val_mae: 0.4143\n",
            "Epoch 45/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.4135 - mae: 0.4135 - val_loss: 0.4132 - val_mae: 0.4132\n",
            "Epoch 46/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.4124 - mae: 0.4124 - val_loss: 0.4121 - val_mae: 0.4121\n",
            "Epoch 47/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.4113 - mae: 0.4113 - val_loss: 0.4110 - val_mae: 0.4110\n",
            "Epoch 48/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.4102 - mae: 0.4102 - val_loss: 0.4099 - val_mae: 0.4099\n",
            "Epoch 49/500\n",
            "117/117 [==============================] - 2s 18ms/step - loss: 0.4090 - mae: 0.4090 - val_loss: 0.4087 - val_mae: 0.4087\n",
            "Epoch 50/500\n",
            "117/117 [==============================] - 2s 18ms/step - loss: 0.4079 - mae: 0.4079 - val_loss: 0.4076 - val_mae: 0.4076\n",
            "Epoch 51/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.4067 - mae: 0.4067 - val_loss: 0.4064 - val_mae: 0.4064\n",
            "Epoch 52/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.4055 - mae: 0.4055 - val_loss: 0.4052 - val_mae: 0.4052\n",
            "Epoch 53/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.4043 - mae: 0.4043 - val_loss: 0.4040 - val_mae: 0.4040\n",
            "Epoch 54/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.4031 - mae: 0.4031 - val_loss: 0.4028 - val_mae: 0.4028\n",
            "Epoch 55/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.4019 - mae: 0.4019 - val_loss: 0.4015 - val_mae: 0.4015\n",
            "Epoch 56/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.4006 - mae: 0.4006 - val_loss: 0.4003 - val_mae: 0.4003\n",
            "Epoch 57/500\n",
            "117/117 [==============================] - 2s 21ms/step - loss: 0.3994 - mae: 0.3994 - val_loss: 0.3990 - val_mae: 0.3990\n",
            "Epoch 58/500\n",
            "117/117 [==============================] - 2s 17ms/step - loss: 0.3981 - mae: 0.3981 - val_loss: 0.3977 - val_mae: 0.3977\n",
            "Epoch 59/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.3968 - mae: 0.3968 - val_loss: 0.3964 - val_mae: 0.3964\n",
            "Epoch 60/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.3955 - mae: 0.3955 - val_loss: 0.3951 - val_mae: 0.3951\n",
            "Epoch 61/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.3941 - mae: 0.3941 - val_loss: 0.3937 - val_mae: 0.3937\n",
            "Epoch 62/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.3928 - mae: 0.3928 - val_loss: 0.3923 - val_mae: 0.3923\n",
            "Epoch 63/500\n",
            "117/117 [==============================] - 1s 13ms/step - loss: 0.3914 - mae: 0.3914 - val_loss: 0.3910 - val_mae: 0.3910\n",
            "Epoch 64/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.3900 - mae: 0.3900 - val_loss: 0.3895 - val_mae: 0.3895\n",
            "Epoch 65/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.3886 - mae: 0.3886 - val_loss: 0.3881 - val_mae: 0.3881\n",
            "Epoch 66/500\n",
            "117/117 [==============================] - 2s 14ms/step - loss: 0.3871 - mae: 0.3871 - val_loss: 0.3867 - val_mae: 0.3867\n",
            "Epoch 67/500\n",
            "117/117 [==============================] - 2s 18ms/step - loss: 0.3857 - mae: 0.3857 - val_loss: 0.3852 - val_mae: 0.3852\n",
            "Epoch 68/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.3842 - mae: 0.3842 - val_loss: 0.3837 - val_mae: 0.3837\n",
            "Epoch 69/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.3827 - mae: 0.3827 - val_loss: 0.3822 - val_mae: 0.3822\n",
            "Epoch 70/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.3812 - mae: 0.3812 - val_loss: 0.3807 - val_mae: 0.3807\n",
            "Epoch 71/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.3796 - mae: 0.3796 - val_loss: 0.3791 - val_mae: 0.3791\n",
            "Epoch 72/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.3780 - mae: 0.3780 - val_loss: 0.3775 - val_mae: 0.3775\n",
            "Epoch 73/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.3764 - mae: 0.3764 - val_loss: 0.3759 - val_mae: 0.3759\n",
            "Epoch 74/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.3748 - mae: 0.3748 - val_loss: 0.3743 - val_mae: 0.3743\n",
            "Epoch 75/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.3732 - mae: 0.3732 - val_loss: 0.3726 - val_mae: 0.3726\n",
            "Epoch 76/500\n",
            "117/117 [==============================] - 2s 17ms/step - loss: 0.3715 - mae: 0.3715 - val_loss: 0.3710 - val_mae: 0.3710\n",
            "Epoch 77/500\n",
            "117/117 [==============================] - 2s 14ms/step - loss: 0.3698 - mae: 0.3698 - val_loss: 0.3692 - val_mae: 0.3692\n",
            "Epoch 78/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.3681 - mae: 0.3681 - val_loss: 0.3675 - val_mae: 0.3675\n",
            "Epoch 79/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.3663 - mae: 0.3663 - val_loss: 0.3658 - val_mae: 0.3658\n",
            "Epoch 80/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.3646 - mae: 0.3646 - val_loss: 0.3640 - val_mae: 0.3640\n",
            "Epoch 81/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.3628 - mae: 0.3628 - val_loss: 0.3622 - val_mae: 0.3622\n",
            "Epoch 82/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.3609 - mae: 0.3609 - val_loss: 0.3603 - val_mae: 0.3603\n",
            "Epoch 83/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.3591 - mae: 0.3591 - val_loss: 0.3584 - val_mae: 0.3584\n",
            "Epoch 84/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.3572 - mae: 0.3572 - val_loss: 0.3566 - val_mae: 0.3566\n",
            "Epoch 85/500\n",
            "117/117 [==============================] - 2s 19ms/step - loss: 0.3553 - mae: 0.3553 - val_loss: 0.3546 - val_mae: 0.3546\n",
            "Epoch 86/500\n",
            "117/117 [==============================] - 2s 15ms/step - loss: 0.3533 - mae: 0.3533 - val_loss: 0.3527 - val_mae: 0.3527\n",
            "Epoch 87/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.3514 - mae: 0.3514 - val_loss: 0.3507 - val_mae: 0.3507\n",
            "Epoch 88/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.3494 - mae: 0.3494 - val_loss: 0.3487 - val_mae: 0.3487\n",
            "Epoch 89/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.3473 - mae: 0.3473 - val_loss: 0.3466 - val_mae: 0.3466\n",
            "Epoch 90/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.3452 - mae: 0.3452 - val_loss: 0.3445 - val_mae: 0.3445\n",
            "Epoch 91/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.3431 - mae: 0.3431 - val_loss: 0.3424 - val_mae: 0.3424\n",
            "Epoch 92/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.3410 - mae: 0.3410 - val_loss: 0.3403 - val_mae: 0.3403\n",
            "Epoch 93/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.3388 - mae: 0.3388 - val_loss: 0.3381 - val_mae: 0.3381\n",
            "Epoch 94/500\n",
            "117/117 [==============================] - 2s 18ms/step - loss: 0.3366 - mae: 0.3366 - val_loss: 0.3359 - val_mae: 0.3359\n",
            "Epoch 95/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.3344 - mae: 0.3344 - val_loss: 0.3336 - val_mae: 0.3336\n",
            "Epoch 96/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.3321 - mae: 0.3321 - val_loss: 0.3313 - val_mae: 0.3313\n",
            "Epoch 97/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.3298 - mae: 0.3298 - val_loss: 0.3290 - val_mae: 0.3290\n",
            "Epoch 98/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.3275 - mae: 0.3275 - val_loss: 0.3267 - val_mae: 0.3267\n",
            "Epoch 99/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.3251 - mae: 0.3251 - val_loss: 0.3243 - val_mae: 0.3243\n",
            "Epoch 100/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.3227 - mae: 0.3227 - val_loss: 0.3219 - val_mae: 0.3219\n",
            "Epoch 101/500\n",
            "117/117 [==============================] - 1s 13ms/step - loss: 0.3203 - mae: 0.3203 - val_loss: 0.3194 - val_mae: 0.3194\n",
            "Epoch 102/500\n",
            "117/117 [==============================] - 2s 14ms/step - loss: 0.3178 - mae: 0.3178 - val_loss: 0.3169 - val_mae: 0.3169\n",
            "Epoch 103/500\n",
            "117/117 [==============================] - 2s 17ms/step - loss: 0.3153 - mae: 0.3153 - val_loss: 0.3144 - val_mae: 0.3144\n",
            "Epoch 104/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.3127 - mae: 0.3127 - val_loss: 0.3118 - val_mae: 0.3118\n",
            "Epoch 105/500\n",
            "117/117 [==============================] - 2s 15ms/step - loss: 0.3101 - mae: 0.3101 - val_loss: 0.3092 - val_mae: 0.3092\n",
            "Epoch 106/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.3075 - mae: 0.3075 - val_loss: 0.3065 - val_mae: 0.3065\n",
            "Epoch 107/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.3048 - mae: 0.3048 - val_loss: 0.3039 - val_mae: 0.3039\n",
            "Epoch 108/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.3021 - mae: 0.3021 - val_loss: 0.3011 - val_mae: 0.3011\n",
            "Epoch 109/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.2994 - mae: 0.2994 - val_loss: 0.2984 - val_mae: 0.2984\n",
            "Epoch 110/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.2966 - mae: 0.2966 - val_loss: 0.2956 - val_mae: 0.2956\n",
            "Epoch 111/500\n",
            "117/117 [==============================] - 2s 15ms/step - loss: 0.2938 - mae: 0.2938 - val_loss: 0.2928 - val_mae: 0.2928\n",
            "Epoch 112/500\n",
            "117/117 [==============================] - 2s 19ms/step - loss: 0.2909 - mae: 0.2909 - val_loss: 0.2899 - val_mae: 0.2899\n",
            "Epoch 113/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.2880 - mae: 0.2880 - val_loss: 0.2870 - val_mae: 0.2870\n",
            "Epoch 114/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.2851 - mae: 0.2851 - val_loss: 0.2840 - val_mae: 0.2840\n",
            "Epoch 115/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.2821 - mae: 0.2821 - val_loss: 0.2811 - val_mae: 0.2811\n",
            "Epoch 116/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.2791 - mae: 0.2791 - val_loss: 0.2780 - val_mae: 0.2780\n",
            "Epoch 117/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.2761 - mae: 0.2761 - val_loss: 0.2750 - val_mae: 0.2750\n",
            "Epoch 118/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.2730 - mae: 0.2730 - val_loss: 0.2719 - val_mae: 0.2719\n",
            "Epoch 119/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.2699 - mae: 0.2699 - val_loss: 0.2688 - val_mae: 0.2688\n",
            "Epoch 120/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.2668 - mae: 0.2668 - val_loss: 0.2656 - val_mae: 0.2656\n",
            "Epoch 121/500\n",
            "117/117 [==============================] - 2s 17ms/step - loss: 0.2636 - mae: 0.2636 - val_loss: 0.2624 - val_mae: 0.2624\n",
            "Epoch 122/500\n",
            "117/117 [==============================] - 2s 15ms/step - loss: 0.2604 - mae: 0.2604 - val_loss: 0.2592 - val_mae: 0.2592\n",
            "Epoch 123/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.2571 - mae: 0.2571 - val_loss: 0.2560 - val_mae: 0.2560\n",
            "Epoch 124/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.2539 - mae: 0.2539 - val_loss: 0.2527 - val_mae: 0.2527\n",
            "Epoch 125/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.2506 - mae: 0.2506 - val_loss: 0.2494 - val_mae: 0.2494\n",
            "Epoch 126/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.2472 - mae: 0.2472 - val_loss: 0.2460 - val_mae: 0.2460\n",
            "Epoch 127/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.2439 - mae: 0.2439 - val_loss: 0.2427 - val_mae: 0.2427\n",
            "Epoch 128/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.2405 - mae: 0.2405 - val_loss: 0.2393 - val_mae: 0.2393\n",
            "Epoch 129/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.2371 - mae: 0.2371 - val_loss: 0.2359 - val_mae: 0.2359\n",
            "Epoch 130/500\n",
            "117/117 [==============================] - 2s 16ms/step - loss: 0.2336 - mae: 0.2336 - val_loss: 0.2324 - val_mae: 0.2324\n",
            "Epoch 131/500\n",
            "117/117 [==============================] - 2s 16ms/step - loss: 0.2302 - mae: 0.2302 - val_loss: 0.2290 - val_mae: 0.2290\n",
            "Epoch 132/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.2267 - mae: 0.2267 - val_loss: 0.2255 - val_mae: 0.2255\n",
            "Epoch 133/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.2232 - mae: 0.2232 - val_loss: 0.2220 - val_mae: 0.2220\n",
            "Epoch 134/500\n",
            "117/117 [==============================] - 2s 16ms/step - loss: 0.2197 - mae: 0.2197 - val_loss: 0.2185 - val_mae: 0.2185\n",
            "Epoch 135/500\n",
            "117/117 [==============================] - 2s 14ms/step - loss: 0.2162 - mae: 0.2162 - val_loss: 0.2149 - val_mae: 0.2149\n",
            "Epoch 136/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.2126 - mae: 0.2126 - val_loss: 0.2114 - val_mae: 0.2114\n",
            "Epoch 137/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.2091 - mae: 0.2091 - val_loss: 0.2078 - val_mae: 0.2078\n",
            "Epoch 138/500\n",
            "117/117 [==============================] - 2s 14ms/step - loss: 0.2055 - mae: 0.2055 - val_loss: 0.2043 - val_mae: 0.2043\n",
            "Epoch 139/500\n",
            "117/117 [==============================] - 2s 16ms/step - loss: 0.2019 - mae: 0.2019 - val_loss: 0.2007 - val_mae: 0.2007\n",
            "Epoch 140/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.1983 - mae: 0.1983 - val_loss: 0.1971 - val_mae: 0.1971\n",
            "Epoch 141/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.1948 - mae: 0.1948 - val_loss: 0.1936 - val_mae: 0.1936\n",
            "Epoch 142/500\n",
            "117/117 [==============================] - 3s 26ms/step - loss: 0.1912 - mae: 0.1912 - val_loss: 0.1900 - val_mae: 0.1900\n",
            "Epoch 143/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.1876 - mae: 0.1876 - val_loss: 0.1864 - val_mae: 0.1864\n",
            "Epoch 144/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.1840 - mae: 0.1840 - val_loss: 0.1828 - val_mae: 0.1828\n",
            "Epoch 145/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.1805 - mae: 0.1805 - val_loss: 0.1793 - val_mae: 0.1793\n",
            "Epoch 146/500\n",
            "117/117 [==============================] - 2s 16ms/step - loss: 0.1769 - mae: 0.1769 - val_loss: 0.1757 - val_mae: 0.1757\n",
            "Epoch 147/500\n",
            "117/117 [==============================] - 2s 15ms/step - loss: 0.1733 - mae: 0.1733 - val_loss: 0.1722 - val_mae: 0.1722\n",
            "Epoch 148/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.1698 - mae: 0.1698 - val_loss: 0.1687 - val_mae: 0.1687\n",
            "Epoch 149/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.1663 - mae: 0.1663 - val_loss: 0.1652 - val_mae: 0.1652\n",
            "Epoch 150/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.1628 - mae: 0.1628 - val_loss: 0.1617 - val_mae: 0.1617\n",
            "Epoch 151/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.1593 - mae: 0.1593 - val_loss: 0.1582 - val_mae: 0.1582\n",
            "Epoch 152/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.1558 - mae: 0.1558 - val_loss: 0.1548 - val_mae: 0.1548\n",
            "Epoch 153/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.1524 - mae: 0.1524 - val_loss: 0.1514 - val_mae: 0.1514\n",
            "Epoch 154/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.1490 - mae: 0.1490 - val_loss: 0.1480 - val_mae: 0.1480\n",
            "Epoch 155/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.1456 - mae: 0.1456 - val_loss: 0.1446 - val_mae: 0.1446\n",
            "Epoch 156/500\n",
            "117/117 [==============================] - 2s 17ms/step - loss: 0.1423 - mae: 0.1423 - val_loss: 0.1413 - val_mae: 0.1413\n",
            "Epoch 157/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.1389 - mae: 0.1389 - val_loss: 0.1380 - val_mae: 0.1380\n",
            "Epoch 158/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.1357 - mae: 0.1357 - val_loss: 0.1348 - val_mae: 0.1348\n",
            "Epoch 159/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.1324 - mae: 0.1324 - val_loss: 0.1316 - val_mae: 0.1316\n",
            "Epoch 160/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.1293 - mae: 0.1293 - val_loss: 0.1284 - val_mae: 0.1284\n",
            "Epoch 161/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.1261 - mae: 0.1261 - val_loss: 0.1253 - val_mae: 0.1253\n",
            "Epoch 162/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.1230 - mae: 0.1230 - val_loss: 0.1222 - val_mae: 0.1222\n",
            "Epoch 163/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.1199 - mae: 0.1199 - val_loss: 0.1192 - val_mae: 0.1192\n",
            "Epoch 164/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.1169 - mae: 0.1169 - val_loss: 0.1162 - val_mae: 0.1162\n",
            "Epoch 165/500\n",
            "117/117 [==============================] - 2s 15ms/step - loss: 0.1140 - mae: 0.1140 - val_loss: 0.1133 - val_mae: 0.1133\n",
            "Epoch 166/500\n",
            "117/117 [==============================] - 2s 15ms/step - loss: 0.1111 - mae: 0.1111 - val_loss: 0.1105 - val_mae: 0.1105\n",
            "Epoch 167/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.1082 - mae: 0.1082 - val_loss: 0.1076 - val_mae: 0.1076\n",
            "Epoch 168/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.1055 - mae: 0.1055 - val_loss: 0.1049 - val_mae: 0.1049\n",
            "Epoch 169/500\n",
            "117/117 [==============================] - 1s 13ms/step - loss: 0.1027 - mae: 0.1027 - val_loss: 0.1022 - val_mae: 0.1022\n",
            "Epoch 170/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.1000 - mae: 0.1000 - val_loss: 0.0995 - val_mae: 0.0995\n",
            "Epoch 171/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0974 - mae: 0.0974 - val_loss: 0.0969 - val_mae: 0.0969\n",
            "Epoch 172/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0948 - mae: 0.0948 - val_loss: 0.0943 - val_mae: 0.0943\n",
            "Epoch 173/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0922 - mae: 0.0922 - val_loss: 0.0918 - val_mae: 0.0918\n",
            "Epoch 174/500\n",
            "117/117 [==============================] - 2s 15ms/step - loss: 0.0898 - mae: 0.0898 - val_loss: 0.0894 - val_mae: 0.0894\n",
            "Epoch 175/500\n",
            "117/117 [==============================] - 2s 18ms/step - loss: 0.0874 - mae: 0.0874 - val_loss: 0.0871 - val_mae: 0.0871\n",
            "Epoch 176/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0850 - mae: 0.0850 - val_loss: 0.0848 - val_mae: 0.0848\n",
            "Epoch 177/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0828 - mae: 0.0828 - val_loss: 0.0825 - val_mae: 0.0825\n",
            "Epoch 178/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0806 - mae: 0.0806 - val_loss: 0.0803 - val_mae: 0.0803\n",
            "Epoch 179/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0784 - mae: 0.0784 - val_loss: 0.0782 - val_mae: 0.0782\n",
            "Epoch 180/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0762 - mae: 0.0762 - val_loss: 0.0761 - val_mae: 0.0761\n",
            "Epoch 181/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0742 - mae: 0.0742 - val_loss: 0.0740 - val_mae: 0.0740\n",
            "Epoch 182/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0721 - mae: 0.0721 - val_loss: 0.0720 - val_mae: 0.0720\n",
            "Epoch 183/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.0701 - mae: 0.0701 - val_loss: 0.0701 - val_mae: 0.0701\n",
            "Epoch 184/500\n",
            "117/117 [==============================] - 2s 20ms/step - loss: 0.0682 - mae: 0.0682 - val_loss: 0.0682 - val_mae: 0.0682\n",
            "Epoch 185/500\n",
            "117/117 [==============================] - 1s 13ms/step - loss: 0.0664 - mae: 0.0664 - val_loss: 0.0664 - val_mae: 0.0664\n",
            "Epoch 186/500\n",
            "117/117 [==============================] - 1s 13ms/step - loss: 0.0646 - mae: 0.0646 - val_loss: 0.0646 - val_mae: 0.0646\n",
            "Epoch 187/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0629 - mae: 0.0629 - val_loss: 0.0629 - val_mae: 0.0629\n",
            "Epoch 188/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0612 - mae: 0.0612 - val_loss: 0.0613 - val_mae: 0.0613\n",
            "Epoch 189/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0595 - mae: 0.0595 - val_loss: 0.0597 - val_mae: 0.0597\n",
            "Epoch 190/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0580 - mae: 0.0580 - val_loss: 0.0581 - val_mae: 0.0581\n",
            "Epoch 191/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0564 - mae: 0.0564 - val_loss: 0.0567 - val_mae: 0.0567\n",
            "Epoch 192/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0550 - mae: 0.0550 - val_loss: 0.0552 - val_mae: 0.0552\n",
            "Epoch 193/500\n",
            "117/117 [==============================] - 2s 16ms/step - loss: 0.0535 - mae: 0.0535 - val_loss: 0.0538 - val_mae: 0.0538\n",
            "Epoch 194/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0522 - mae: 0.0522 - val_loss: 0.0525 - val_mae: 0.0525\n",
            "Epoch 195/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0508 - mae: 0.0508 - val_loss: 0.0512 - val_mae: 0.0512\n",
            "Epoch 196/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0496 - mae: 0.0496 - val_loss: 0.0499 - val_mae: 0.0499\n",
            "Epoch 197/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0483 - mae: 0.0483 - val_loss: 0.0487 - val_mae: 0.0487\n",
            "Epoch 198/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0472 - mae: 0.0472 - val_loss: 0.0476 - val_mae: 0.0476\n",
            "Epoch 199/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0460 - mae: 0.0460 - val_loss: 0.0465 - val_mae: 0.0465\n",
            "Epoch 200/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0449 - mae: 0.0449 - val_loss: 0.0454 - val_mae: 0.0454\n",
            "Epoch 201/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0439 - mae: 0.0439 - val_loss: 0.0444 - val_mae: 0.0444\n",
            "Epoch 202/500\n",
            "117/117 [==============================] - 2s 16ms/step - loss: 0.0428 - mae: 0.0428 - val_loss: 0.0434 - val_mae: 0.0434\n",
            "Epoch 203/500\n",
            "117/117 [==============================] - 2s 15ms/step - loss: 0.0418 - mae: 0.0418 - val_loss: 0.0424 - val_mae: 0.0424\n",
            "Epoch 204/500\n",
            "117/117 [==============================] - 1s 13ms/step - loss: 0.0409 - mae: 0.0409 - val_loss: 0.0415 - val_mae: 0.0415\n",
            "Epoch 205/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0400 - mae: 0.0400 - val_loss: 0.0406 - val_mae: 0.0406\n",
            "Epoch 206/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0392 - mae: 0.0392 - val_loss: 0.0398 - val_mae: 0.0398\n",
            "Epoch 207/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0384 - mae: 0.0384 - val_loss: 0.0391 - val_mae: 0.0391\n",
            "Epoch 208/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0376 - mae: 0.0376 - val_loss: 0.0384 - val_mae: 0.0384\n",
            "Epoch 209/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0369 - mae: 0.0369 - val_loss: 0.0377 - val_mae: 0.0377\n",
            "Epoch 210/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0363 - mae: 0.0363 - val_loss: 0.0370 - val_mae: 0.0370\n",
            "Epoch 211/500\n",
            "117/117 [==============================] - 2s 16ms/step - loss: 0.0356 - mae: 0.0356 - val_loss: 0.0364 - val_mae: 0.0364\n",
            "Epoch 212/500\n",
            "117/117 [==============================] - 2s 14ms/step - loss: 0.0350 - mae: 0.0350 - val_loss: 0.0358 - val_mae: 0.0358\n",
            "Epoch 213/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0344 - mae: 0.0344 - val_loss: 0.0352 - val_mae: 0.0352\n",
            "Epoch 214/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0339 - mae: 0.0339 - val_loss: 0.0347 - val_mae: 0.0347\n",
            "Epoch 215/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0333 - mae: 0.0333 - val_loss: 0.0341 - val_mae: 0.0341\n",
            "Epoch 216/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0327 - mae: 0.0327 - val_loss: 0.0336 - val_mae: 0.0336\n",
            "Epoch 217/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0322 - mae: 0.0322 - val_loss: 0.0330 - val_mae: 0.0330\n",
            "Epoch 218/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0317 - mae: 0.0317 - val_loss: 0.0325 - val_mae: 0.0325\n",
            "Epoch 219/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0312 - mae: 0.0312 - val_loss: 0.0321 - val_mae: 0.0321\n",
            "Epoch 220/500\n",
            "117/117 [==============================] - 2s 14ms/step - loss: 0.0307 - mae: 0.0307 - val_loss: 0.0316 - val_mae: 0.0316\n",
            "Epoch 221/500\n",
            "117/117 [==============================] - 2s 16ms/step - loss: 0.0303 - mae: 0.0303 - val_loss: 0.0312 - val_mae: 0.0312\n",
            "Epoch 222/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0299 - mae: 0.0299 - val_loss: 0.0308 - val_mae: 0.0308\n",
            "Epoch 223/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0295 - mae: 0.0295 - val_loss: 0.0304 - val_mae: 0.0304\n",
            "Epoch 224/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0292 - mae: 0.0292 - val_loss: 0.0301 - val_mae: 0.0301\n",
            "Epoch 225/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0289 - mae: 0.0289 - val_loss: 0.0298 - val_mae: 0.0298\n",
            "Epoch 226/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0286 - mae: 0.0286 - val_loss: 0.0296 - val_mae: 0.0296\n",
            "Epoch 227/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0283 - mae: 0.0283 - val_loss: 0.0293 - val_mae: 0.0293\n",
            "Epoch 228/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0281 - mae: 0.0281 - val_loss: 0.0291 - val_mae: 0.0291\n",
            "Epoch 229/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0278 - mae: 0.0278 - val_loss: 0.0289 - val_mae: 0.0289\n",
            "Epoch 230/500\n",
            "117/117 [==============================] - 2s 16ms/step - loss: 0.0276 - mae: 0.0276 - val_loss: 0.0286 - val_mae: 0.0286\n",
            "Epoch 231/500\n",
            "117/117 [==============================] - 2s 14ms/step - loss: 0.0274 - mae: 0.0274 - val_loss: 0.0284 - val_mae: 0.0284\n",
            "Epoch 232/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0272 - mae: 0.0272 - val_loss: 0.0282 - val_mae: 0.0282\n",
            "Epoch 233/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0270 - mae: 0.0270 - val_loss: 0.0281 - val_mae: 0.0281\n",
            "Epoch 234/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0269 - mae: 0.0269 - val_loss: 0.0279 - val_mae: 0.0279\n",
            "Epoch 235/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0267 - mae: 0.0267 - val_loss: 0.0277 - val_mae: 0.0277\n",
            "Epoch 236/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0265 - mae: 0.0265 - val_loss: 0.0275 - val_mae: 0.0275\n",
            "Epoch 237/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0263 - mae: 0.0263 - val_loss: 0.0273 - val_mae: 0.0273\n",
            "Epoch 238/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0262 - mae: 0.0262 - val_loss: 0.0272 - val_mae: 0.0272\n",
            "Epoch 239/500\n",
            "117/117 [==============================] - 2s 16ms/step - loss: 0.0260 - mae: 0.0260 - val_loss: 0.0270 - val_mae: 0.0270\n",
            "Epoch 240/500\n",
            "117/117 [==============================] - 3s 27ms/step - loss: 0.0258 - mae: 0.0258 - val_loss: 0.0269 - val_mae: 0.0269\n",
            "Epoch 241/500\n",
            "117/117 [==============================] - 3s 24ms/step - loss: 0.0257 - mae: 0.0257 - val_loss: 0.0267 - val_mae: 0.0267\n",
            "Epoch 242/500\n",
            "117/117 [==============================] - 3s 23ms/step - loss: 0.0256 - mae: 0.0256 - val_loss: 0.0266 - val_mae: 0.0266\n",
            "Epoch 243/500\n",
            "117/117 [==============================] - 3s 25ms/step - loss: 0.0254 - mae: 0.0254 - val_loss: 0.0264 - val_mae: 0.0264\n",
            "Epoch 244/500\n",
            "117/117 [==============================] - 3s 26ms/step - loss: 0.0253 - mae: 0.0253 - val_loss: 0.0263 - val_mae: 0.0263\n",
            "Epoch 245/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0252 - mae: 0.0252 - val_loss: 0.0262 - val_mae: 0.0262\n",
            "Epoch 246/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0251 - mae: 0.0251 - val_loss: 0.0261 - val_mae: 0.0261\n",
            "Epoch 247/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0250 - mae: 0.0250 - val_loss: 0.0260 - val_mae: 0.0260\n",
            "Epoch 248/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0249 - mae: 0.0249 - val_loss: 0.0259 - val_mae: 0.0259\n",
            "Epoch 249/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0248 - mae: 0.0248 - val_loss: 0.0258 - val_mae: 0.0258\n",
            "Epoch 250/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0247 - mae: 0.0247 - val_loss: 0.0258 - val_mae: 0.0258\n",
            "Epoch 251/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0246 - mae: 0.0246 - val_loss: 0.0257 - val_mae: 0.0257\n",
            "Epoch 252/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0246 - mae: 0.0246 - val_loss: 0.0256 - val_mae: 0.0256\n",
            "Epoch 253/500\n",
            "117/117 [==============================] - 2s 15ms/step - loss: 0.0245 - mae: 0.0245 - val_loss: 0.0255 - val_mae: 0.0255\n",
            "Epoch 254/500\n",
            "117/117 [==============================] - 2s 16ms/step - loss: 0.0244 - mae: 0.0244 - val_loss: 0.0255 - val_mae: 0.0255\n",
            "Epoch 255/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0244 - mae: 0.0244 - val_loss: 0.0254 - val_mae: 0.0254\n",
            "Epoch 256/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0243 - mae: 0.0243 - val_loss: 0.0253 - val_mae: 0.0253\n",
            "Epoch 257/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0242 - mae: 0.0242 - val_loss: 0.0253 - val_mae: 0.0253\n",
            "Epoch 258/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0242 - mae: 0.0242 - val_loss: 0.0252 - val_mae: 0.0252\n",
            "Epoch 259/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0241 - mae: 0.0241 - val_loss: 0.0251 - val_mae: 0.0251\n",
            "Epoch 260/500\n",
            "117/117 [==============================] - 1s 13ms/step - loss: 0.0241 - mae: 0.0241 - val_loss: 0.0251 - val_mae: 0.0251\n",
            "Epoch 261/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.0240 - mae: 0.0240 - val_loss: 0.0250 - val_mae: 0.0250\n",
            "Epoch 262/500\n",
            "117/117 [==============================] - 2s 19ms/step - loss: 0.0239 - mae: 0.0239 - val_loss: 0.0250 - val_mae: 0.0250\n",
            "Epoch 263/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.0239 - mae: 0.0239 - val_loss: 0.0249 - val_mae: 0.0249\n",
            "Epoch 264/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0238 - mae: 0.0238 - val_loss: 0.0248 - val_mae: 0.0248\n",
            "Epoch 265/500\n",
            "117/117 [==============================] - 1s 13ms/step - loss: 0.0238 - mae: 0.0238 - val_loss: 0.0248 - val_mae: 0.0248\n",
            "Epoch 266/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0237 - mae: 0.0237 - val_loss: 0.0247 - val_mae: 0.0247\n",
            "Epoch 267/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0237 - mae: 0.0237 - val_loss: 0.0247 - val_mae: 0.0247\n",
            "Epoch 268/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0236 - mae: 0.0236 - val_loss: 0.0246 - val_mae: 0.0246\n",
            "Epoch 269/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0236 - mae: 0.0236 - val_loss: 0.0246 - val_mae: 0.0246\n",
            "Epoch 270/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0235 - mae: 0.0235 - val_loss: 0.0246 - val_mae: 0.0246\n",
            "Epoch 271/500\n",
            "117/117 [==============================] - 2s 17ms/step - loss: 0.0235 - mae: 0.0235 - val_loss: 0.0245 - val_mae: 0.0245\n",
            "Epoch 272/500\n",
            "117/117 [==============================] - 2s 16ms/step - loss: 0.0235 - mae: 0.0235 - val_loss: 0.0245 - val_mae: 0.0245\n",
            "Epoch 273/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0234 - mae: 0.0234 - val_loss: 0.0244 - val_mae: 0.0244\n",
            "Epoch 274/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0234 - mae: 0.0234 - val_loss: 0.0244 - val_mae: 0.0244\n",
            "Epoch 275/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0234 - mae: 0.0234 - val_loss: 0.0244 - val_mae: 0.0244\n",
            "Epoch 276/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0243 - val_mae: 0.0243\n",
            "Epoch 277/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0243 - val_mae: 0.0243\n",
            "Epoch 278/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0233 - mae: 0.0233 - val_loss: 0.0243 - val_mae: 0.0243\n",
            "Epoch 279/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0232 - mae: 0.0232 - val_loss: 0.0242 - val_mae: 0.0242\n",
            "Epoch 280/500\n",
            "117/117 [==============================] - 1s 13ms/step - loss: 0.0232 - mae: 0.0232 - val_loss: 0.0242 - val_mae: 0.0242\n",
            "Epoch 281/500\n",
            "117/117 [==============================] - 2s 18ms/step - loss: 0.0232 - mae: 0.0232 - val_loss: 0.0242 - val_mae: 0.0242\n",
            "Epoch 282/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0231 - mae: 0.0231 - val_loss: 0.0241 - val_mae: 0.0241\n",
            "Epoch 283/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0231 - mae: 0.0231 - val_loss: 0.0241 - val_mae: 0.0241\n",
            "Epoch 284/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0231 - mae: 0.0231 - val_loss: 0.0241 - val_mae: 0.0241\n",
            "Epoch 285/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0231 - mae: 0.0231 - val_loss: 0.0241 - val_mae: 0.0241\n",
            "Epoch 286/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0240 - val_mae: 0.0240\n",
            "Epoch 287/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0240 - val_mae: 0.0240\n",
            "Epoch 288/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0240 - val_mae: 0.0240\n",
            "Epoch 289/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0230 - mae: 0.0230 - val_loss: 0.0240 - val_mae: 0.0240\n",
            "Epoch 290/500\n",
            "117/117 [==============================] - 2s 20ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0239 - val_mae: 0.0239\n",
            "Epoch 291/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0239 - val_mae: 0.0239\n",
            "Epoch 292/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0239 - val_mae: 0.0239\n",
            "Epoch 293/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0239 - val_mae: 0.0239\n",
            "Epoch 294/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0229 - mae: 0.0229 - val_loss: 0.0238 - val_mae: 0.0238\n",
            "Epoch 295/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0238 - val_mae: 0.0238\n",
            "Epoch 296/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0238 - val_mae: 0.0238\n",
            "Epoch 297/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0238 - val_mae: 0.0238\n",
            "Epoch 298/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0238 - val_mae: 0.0238\n",
            "Epoch 299/500\n",
            "117/117 [==============================] - 2s 20ms/step - loss: 0.0228 - mae: 0.0228 - val_loss: 0.0237 - val_mae: 0.0237\n",
            "Epoch 300/500\n",
            "117/117 [==============================] - 2s 14ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0237 - val_mae: 0.0237\n",
            "Epoch 301/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0237 - val_mae: 0.0237\n",
            "Epoch 302/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0237 - val_mae: 0.0237\n",
            "Epoch 303/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0237 - val_mae: 0.0237\n",
            "Epoch 304/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0237 - val_mae: 0.0237\n",
            "Epoch 305/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0237 - val_mae: 0.0237\n",
            "Epoch 306/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0227 - mae: 0.0227 - val_loss: 0.0236 - val_mae: 0.0236\n",
            "Epoch 307/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0236 - val_mae: 0.0236\n",
            "Epoch 308/500\n",
            "117/117 [==============================] - 2s 15ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0236 - val_mae: 0.0236\n",
            "Epoch 309/500\n",
            "117/117 [==============================] - 2s 18ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0236 - val_mae: 0.0236\n",
            "Epoch 310/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0236 - val_mae: 0.0236\n",
            "Epoch 311/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0236 - val_mae: 0.0236\n",
            "Epoch 312/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0236 - val_mae: 0.0236\n",
            "Epoch 313/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0236 - val_mae: 0.0236\n",
            "Epoch 314/500\n",
            "117/117 [==============================] - 2s 18ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0236 - val_mae: 0.0236\n",
            "Epoch 315/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0235 - val_mae: 0.0235\n",
            "Epoch 316/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0235 - val_mae: 0.0235\n",
            "Epoch 317/500\n",
            "117/117 [==============================] - 2s 19ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0235 - val_mae: 0.0235\n",
            "Epoch 318/500\n",
            "117/117 [==============================] - 2s 14ms/step - loss: 0.0226 - mae: 0.0226 - val_loss: 0.0235 - val_mae: 0.0235\n",
            "Epoch 319/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0235 - val_mae: 0.0235\n",
            "Epoch 320/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0235 - val_mae: 0.0235\n",
            "Epoch 321/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0235 - val_mae: 0.0235\n",
            "Epoch 322/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0235 - val_mae: 0.0235\n",
            "Epoch 323/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0235 - val_mae: 0.0235\n",
            "Epoch 324/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0235 - val_mae: 0.0235\n",
            "Epoch 325/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0235 - val_mae: 0.0235\n",
            "Epoch 326/500\n",
            "117/117 [==============================] - 2s 17ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0235 - val_mae: 0.0235\n",
            "Epoch 327/500\n",
            "117/117 [==============================] - 2s 15ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0235 - val_mae: 0.0235\n",
            "Epoch 328/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0235 - val_mae: 0.0235\n",
            "Epoch 329/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0235 - val_mae: 0.0235\n",
            "Epoch 330/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0235 - val_mae: 0.0235\n",
            "Epoch 331/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0235 - val_mae: 0.0235\n",
            "Epoch 332/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0235 - val_mae: 0.0235\n",
            "Epoch 333/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0235 - val_mae: 0.0235\n",
            "Epoch 334/500\n",
            "117/117 [==============================] - 2s 14ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0235 - val_mae: 0.0235\n",
            "Epoch 335/500\n",
            "117/117 [==============================] - 2s 20ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0235 - val_mae: 0.0235\n",
            "Epoch 336/500\n",
            "117/117 [==============================] - 2s 14ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0235 - val_mae: 0.0235\n",
            "Epoch 337/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0235 - val_mae: 0.0235\n",
            "Epoch 338/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0235 - val_mae: 0.0235\n",
            "Epoch 339/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0235 - val_mae: 0.0235\n",
            "Epoch 340/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 341/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 342/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 343/500\n",
            "117/117 [==============================] - 2s 15ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 344/500\n",
            "117/117 [==============================] - 2s 16ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 345/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 346/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 347/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 348/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 349/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 350/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 351/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 352/500\n",
            "117/117 [==============================] - 2s 16ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 353/500\n",
            "117/117 [==============================] - 2s 20ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 354/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 355/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 356/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 357/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 358/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 359/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 360/500\n",
            "117/117 [==============================] - 2s 14ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 361/500\n",
            "117/117 [==============================] - 2s 18ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 362/500\n",
            "117/117 [==============================] - 1s 13ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 363/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 364/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 365/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 366/500\n",
            "117/117 [==============================] - 1s 13ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 367/500\n",
            "117/117 [==============================] - 1s 13ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 368/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 369/500\n",
            "117/117 [==============================] - 2s 17ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 370/500\n",
            "117/117 [==============================] - 2s 15ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 371/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 372/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 373/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 374/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 375/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 376/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 377/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 378/500\n",
            "117/117 [==============================] - 2s 18ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 379/500\n",
            "117/117 [==============================] - 2s 18ms/step - loss: 0.0225 - mae: 0.0225 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 380/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 381/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 382/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 383/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 384/500\n",
            "117/117 [==============================] - 2s 17ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 385/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 386/500\n",
            "117/117 [==============================] - 2s 17ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 387/500\n",
            "117/117 [==============================] - 2s 18ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 388/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 389/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 390/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 391/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 392/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 393/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 394/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 395/500\n",
            "117/117 [==============================] - 3s 28ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 396/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 397/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 398/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 399/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 400/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 401/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 402/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 403/500\n",
            "117/117 [==============================] - 2s 15ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 404/500\n",
            "117/117 [==============================] - 2s 19ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 405/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 406/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 407/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 408/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 409/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 410/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 411/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 412/500\n",
            "117/117 [==============================] - 2s 15ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 413/500\n",
            "117/117 [==============================] - 2s 19ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 414/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 415/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 416/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 417/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 418/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 419/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 420/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 421/500\n",
            "117/117 [==============================] - 2s 14ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 422/500\n",
            "117/117 [==============================] - 2s 18ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 423/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 424/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 425/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 426/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 427/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 428/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 429/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 430/500\n",
            "117/117 [==============================] - 2s 20ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 431/500\n",
            "117/117 [==============================] - 2s 15ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 432/500\n",
            "117/117 [==============================] - 1s 13ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 433/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 434/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 435/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 436/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 437/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 438/500\n",
            "117/117 [==============================] - 2s 14ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 439/500\n",
            "117/117 [==============================] - 2s 17ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 440/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 441/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 442/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 443/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 444/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 445/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 446/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 447/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 448/500\n",
            "117/117 [==============================] - 2s 18ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 449/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 450/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 451/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 452/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 453/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 454/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 455/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 456/500\n",
            "117/117 [==============================] - 2s 14ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 457/500\n",
            "117/117 [==============================] - 2s 17ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 458/500\n",
            "117/117 [==============================] - 2s 14ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 459/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 460/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 461/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 462/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 463/500\n",
            "117/117 [==============================] - 1s 10ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 464/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 465/500\n",
            "117/117 [==============================] - 2s 19ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 466/500\n",
            "117/117 [==============================] - 2s 20ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 467/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 468/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 469/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 470/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 471/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 472/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 473/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 474/500\n",
            "117/117 [==============================] - 2s 19ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 475/500\n",
            "117/117 [==============================] - 2s 15ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 476/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 477/500\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 478/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 479/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 480/500\n",
            "117/117 [==============================] - 2s 14ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 481/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 482/500\n",
            "117/117 [==============================] - 2s 18ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 483/500\n",
            "117/117 [==============================] - 2s 19ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 484/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 485/500\n",
            "117/117 [==============================] - 1s 13ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 486/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 487/500\n",
            "117/117 [==============================] - 2s 14ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 488/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 489/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 490/500\n",
            "117/117 [==============================] - 2s 15ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 491/500\n",
            "117/117 [==============================] - 2s 18ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 492/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 493/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 494/500\n",
            "117/117 [==============================] - 1s 11ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 495/500\n",
            "117/117 [==============================] - 2s 14ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 496/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 497/500\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 498/500\n",
            "117/117 [==============================] - 2s 14ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 499/500\n",
            "117/117 [==============================] - 2s 17ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n",
            "Epoch 500/500\n",
            "117/117 [==============================] - 2s 16ms/step - loss: 0.0224 - mae: 0.0224 - val_loss: 0.0234 - val_mae: 0.0234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Extract data from the training history\n",
        "training_loss = history.history['loss']\n",
        "validation_loss = history.history['val_loss']\n",
        "epochs = range(1, len(training_loss) + 1)\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "# Plot training and validation loss\n",
        "plt.plot(epochs, training_loss, label='Training Loss', marker='o')\n",
        "plt.plot(epochs, validation_loss, label='Validation Loss', marker='o')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "emzYPn9_FVJO",
        "outputId": "5fc64e34-faeb-4252-d2ac-c160177910fa"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}